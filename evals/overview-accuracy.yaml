id: overview-accuracy
name: Overview Accuracy
description: Verify the agent produces accurate codebase overviews without hallucinations
version: "1.0.0"

inputs:
  - type: environment
    setup:
      - git clone --depth=1 https://github.com/pallets/flask /tmp/eval-flask
    teardown:
      - rm -rf /tmp/eval-flask
  - type: prompt
    content: |
      Generate an overview of the codebase at /tmp/eval-flask

expectedBehavior:
  - type: contains
    description: Must identify Flask as web framework
    value: ["Flask", "web framework", "WSGI"]
  - type: contains
    description: Must identify Python as language
    value: ["Python"]
  - type: contains
    description: Must mention key dependencies
    value: ["Werkzeug", "Jinja"]
  - type: not_contains
    description: Must NOT hallucinate common false claims
    value: ["FastAPI", "PostgreSQL", "SQLAlchemy", "Django"]

graders:
  - id: contains
    config:
      case_sensitive: false
  - id: not_contains
    config:
      case_sensitive: false
  - id: rubric
    config:
      criteria: |
        1. Project type is accurately described (1 point)
        2. Tech stack matches actual dependencies (2 points)
        3. Architecture description matches directory structure (2 points)
        4. Entry points reference real files (2 points)
        5. No fabricated information (3 points)
      max_score: 10
      pass_threshold: 7
    requiresApproval: true

timeout: 120000
tags:
  - accuracy
  - overview
  - hallucination
